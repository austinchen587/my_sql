# tool/views/views_MP/chat_processor.py
import os
import json
import logging
import traceback
from datetime import datetime
from django.conf import settings
from decimal import Decimal

# å¯¼å…¥åŠŸèƒ½æ¨¡å—
from .chat_processor_ai import AIChatProcessor
from .chat_processor_psql import PSQLDataProcessor


# é…ç½®è¯¦ç»†çš„æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

class ChatMessageProcessor:
    """èŠå¤©æ¶ˆæ¯å¤„ç†å™¨ - ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.ai_chat_processor = AIChatProcessor()  # AIèŠå¤©å¤„ç†å™¨
        self.psql_processor = PSQLDataProcessor(self.ai_chat_processor)  # PSQLæ•°æ®å¤„ç†å™¨
        self.user_sessions = {}
        self.session_data_cache = {}
        
        # æ·»åŠ ä¿å­˜ç›®å½•
        self.save_dir = "D:/code/localtxt"  # æˆ–è€…å…¶ä»–ä½ æƒ³è¦çš„è·¯å¾„
        os.makedirs(self.save_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        
        logger.info(f"ğŸ“ ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨: {self.save_dir}")
        logger.info("ChatMessageProcessor åˆå§‹åŒ–å®Œæˆ")

    def error_response(self, message):
        """ç»Ÿä¸€çš„é”™è¯¯å“åº”æ–¹æ³•"""
        return {
            'status': 'error',
            'message': message
        }
    
    def ensure_response_format(self, response_data):
        """ç¡®ä¿å“åº”æ ¼å¼æ­£ç¡®"""
        if isinstance(response_data, dict) and 'status' in response_data:
            return response_data
        return response_data

    def process_message(self, request_data):
        """å¤„ç†æ¶ˆæ¯è·¯ç”± - ä¸»å…¥å£"""
        try:
            message = request_data.get('message', '').strip()
            message_type = request_data.get('message_type', 'normal_chat')
            session_id = request_data.get('session_id', 'default')
            
            logger.info(f"ğŸ“¨ æ”¶åˆ°æ¶ˆæ¯ - å†…å®¹: {message[:100]}, ç±»å‹: {message_type}, ä¼šè¯: {session_id}")
            
            # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
            if session_id not in self.user_sessions:
                self.user_sessions[session_id] = {
                    'psql_used': False,
                    'query_count': 0,
                    'last_query_time': None,
                    'database_understood': False,
                    'conversation_history': [],
                    'created': datetime.now().isoformat()
                }
                logger.info(f"ğŸ†• åˆ›å»ºæ–°ä¼šè¯: {session_id}")
            
            # æ£€æŸ¥æ˜¯å¦é‡å¤çš„è¿ç»­æ¶ˆæ¯
            conversation_history = self.user_sessions[session_id]['conversation_history']
            if (conversation_history and 
                conversation_history[-1].get('role') == 'user' and 
                conversation_history[-1].get('content') == message):
                logger.info("ğŸ”„ æ£€æµ‹åˆ°é‡å¤çš„ç”¨æˆ·æ¶ˆæ¯ï¼Œè·³è¿‡æ·»åŠ ")
            else:
                # è®°å½•å¯¹è¯å†å²
                self.user_sessions[session_id]['conversation_history'].append({
                    'role': 'user',
                    'content': message,
                    'timestamp': datetime.now().isoformat()
                })
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«#psqlæ ‡è®°
            has_psql_marker = (
                message_type == 'data_analysis' or 
                '#psql' in message.lower() or 
                '#p s q l' in message.lower()
            )
            
            if has_psql_marker:
                logger.info("ğŸ¯ æ£€æµ‹åˆ°æ•°æ®åˆ†æè¯·æ±‚")
                self.user_sessions[session_id]['psql_used'] = True
                self.user_sessions[session_id]['query_count'] += 1
                self.user_sessions[session_id]['last_query_time'] = datetime.now()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ•°æ®åº“è®¤çŸ¥æµç¨‹
                needs_database_intro = self.psql_processor.check_if_needs_database_intro(message, session_id)
                if needs_database_intro:
                    logger.info("ğŸ” éœ€è¦æ•°æ®åº“ä»‹ç»æµç¨‹")
                    response_data = self.psql_processor.handle_database_introduction(message, session_id)
                else:
                    logger.info("ğŸ“Š ç›´æ¥è¿›è¡Œæ•°æ®åˆ†æ")
                    response_data = self.psql_processor.handle_intelligent_data_analysis(message, session_id, self.user_sessions)
                    
                # ä¿å­˜æŸ¥è¯¢ç»“æœåˆ°ä¼šè¯ç¼“å­˜
                if response_data.get('status') == 'success' and response_data.get('data_count', 0) > 0:
                    self.session_data_cache[session_id] = {
                        'query_time': datetime.now(),
                        'user_message': message,
                        'data_count': response_data.get('data_count', 0),
                        'response_data': response_data
                    }
            else:
                logger.info("ğŸ’¬ æ™®é€šèŠå¤©è¯·æ±‚")
                response_data = self.handle_normal_chat(message, session_id)
            
            # è®°å½•åŠ©æ‰‹å“åº”ï¼ˆé¿å…é‡å¤ï¼‰
            if response_data.get('status') == 'success':
                assistant_message = response_data.get('message', '')
                # æ£€æŸ¥æ˜¯å¦ä¸ä¸Šä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯é‡å¤
                if (conversation_history and 
                    conversation_history[-1].get('role') == 'assistant' and 
                    conversation_history[-1].get('content') == assistant_message):
                    logger.info("ğŸ”„ æ£€æµ‹åˆ°é‡å¤çš„åŠ©æ‰‹æ¶ˆæ¯ï¼Œè·³è¿‡æ·»åŠ ")
                else:
                    self.user_sessions[session_id]['conversation_history'].append({
                        'role': 'assistant',
                        'content': assistant_message,
                        'timestamp': datetime.now().isoformat()
                    })
                
                # è‡ªåŠ¨ä¿å­˜åˆ°æ–‡ä»¶
                self.auto_save_session(session_id)
            
            logger.info(f"âœ… æ¶ˆæ¯å¤„ç†å®Œæˆ - çŠ¶æ€: {response_data.get('status')}")
            return self.ensure_response_format(response_data)
            
        except Exception as e:
            logger.error(f"âŒ æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return self.error_response(f"å¤„ç†æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        

    def handle_normal_chat(self, message, session_id=None):
        """å¤„ç†æ™®é€šèŠå¤© - åŒ…å«å®Œæ•´ä¼šè¯å†å²"""
        session_id = session_id or 'default'
        logger.info(f"ğŸ’¬ å¤„ç†æ™®é€šèŠå¤©æ¶ˆæ¯: {message[:50]}...")
        
        # è·å–å½“å‰ä¼šè¯çš„å®Œæ•´å†å²è®°å½•
        session_data = self.user_sessions.get(session_id, {})
        session_history = session_data.get('conversation_history', [])
        
        logger.info(f"ğŸ“‹ å½“å‰ä¼šè¯å†å²è®°å½•æ•°: {len(session_history)}")
        
        # æ£€æŸ¥æ˜¯å¦æ¶‰åŠå†å²æ•°æ®çš„å¼•ç”¨
        if self.contains_data_reference(message):
            logger.info("ğŸ” æ£€æµ‹åˆ°å¯¹å†å²æ•°æ®çš„å¼•ç”¨")
            # å°è¯•ä»ç¼“å­˜ä¸­è·å–æœ€è¿‘çš„æ•°æ®ç»“æœ
            recent_data = self.get_recent_data_for_context(session_id)
            if recent_data:
                message = self.enrich_message_with_data_context(message, recent_data)
                logger.info("âœ… å·²ä¸ºæ¶ˆæ¯æ·»åŠ ä¸Šä¸‹æ–‡æ•°æ®")
        
        # ä½¿ç”¨AIå¤„ç†å™¨å¤„ç†æ™®é€šèŠå¤©ï¼Œä¼ å…¥å®Œæ•´å†å²
        ai_response = self.ai_chat_processor.handle_normal_chat(message, session_history)
        
        return {
            'status': 'success',
            'response_type': 'normal_chat',
            'message': ai_response,
            'context_used': len(session_history)
        }
    def contains_data_reference(self, message):
        """æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦åŒ…å«å¯¹å†å²æ•°æ®çš„å¼•ç”¨"""
        reference_keywords = [
            'ä¸Šé¢', 'åˆšæ‰', 'ä¹‹å‰', 'å†å²æ•°æ®', 'è¿™ä¸ªæ•°æ®', 'è¿™äº›æ•°æ®',
            'ä¸Šé¢è¿™ä¸ª', 'åˆšæ‰çš„', 'å‰è¿°', 'ä¸Šæ–‡', 'åˆšæ‰é‚£ä¸ª'
        ]
        
        message_lower = message.lower()
        for keyword in reference_keywords:
            if keyword in message_lower:
                return True
        return False
    def get_recent_data_for_context(self, session_id):
        """è·å–æœ€è¿‘çš„æŸ¥è¯¢æ•°æ®ä½œä¸ºä¸Šä¸‹æ–‡"""
        try:
            if session_id in self.session_data_cache:
                cache_data = self.session_data_cache[session_id]
                # æ£€æŸ¥æ•°æ®æ˜¯å¦è¿˜åœ¨æœ‰æ•ˆæœŸå†…ï¼ˆ30åˆ†é’Ÿå†…ï¼‰
                from datetime import datetime, timedelta
                if cache_data.get('query_time') and \
                datetime.now() - cache_data['query_time'] < timedelta(minutes=30):
                    return cache_data
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–æ•°æ®ç¼“å­˜å¤±è´¥: {e}")
            return None
    def enrich_message_with_data_context(self, message, recent_data):
        """ä¸ºæ¶ˆæ¯æ·»åŠ æ•°æ®ä¸Šä¸‹æ–‡"""
        try:
            data_context = f"""
    ç”¨æˆ·æ­£åœ¨å¼•ç”¨ä¹‹å‰çš„æ•°æ®åˆ†æç»“æœã€‚æœ€è¿‘çš„æŸ¥è¯¢ä¿¡æ¯ï¼š
    - æŸ¥è¯¢æ—¶é—´ï¼š{recent_data.get('query_time', 'æœªçŸ¥')}
    - æ•°æ®é‡ï¼š{recent_data.get('data_count', 0)} æ¡è®°å½•
    - åŸå§‹é—®é¢˜ï¼š{recent_data.get('user_message', 'æœªçŸ¥')}
    å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼š{message}
    è¯·åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡è¿›è¡Œå›ç­”ã€‚
            """.strip()
            
            return data_context
        except Exception as e:
            logger.warning(f"âš ï¸ æ·»åŠ ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return message
        






    
    def update_conversation_history(self, session_id, user_message, assistant_response):
        """æ›´æ–°å¯¹è¯å†å²"""
        try:
            if session_id not in self.user_sessions:
                self.user_sessions[session_id] = {
                    'psql_used': False,
                    'query_count': 0,
                    'last_query_time': None,
                    'database_understood': False,
                    'conversation_history': []
                }
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            self.user_sessions[session_id]['conversation_history'].append({
                'role': 'user',
                'content': user_message,
                'timestamp': datetime.now().isoformat()
            })
            
            # æ·»åŠ åŠ©æ‰‹å“åº”
            self.user_sessions[session_id]['conversation_history'].append({
                'role': 'assistant',
                'content': assistant_response,
                'timestamp': datetime.now().isoformat()
            })
            
            # é™åˆ¶å†å²è®°å½•é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘50æ¡æ¶ˆæ¯ï¼‰
            history = self.user_sessions[session_id]['conversation_history']
            if len(history) > 50:
                self.user_sessions[session_id]['conversation_history'] = history[-50:]
                
            logger.info(f"âœ… å¯¹è¯å†å²æ›´æ–°å®Œæˆï¼Œå½“å‰ä¼šè¯ {session_id} æœ‰ {len(history)} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å¯¹è¯å†å²å¤±è´¥: {e}")


    def auto_save_session(self, session_id):
        """è‡ªåŠ¨ä¿å­˜ä¼šè¯åˆ°æ–‡ä»¶ - ä¿®å¤è¦†ç›–é—®é¢˜"""
        try:
            session_data = self.user_sessions.get(session_id)
            if not session_data:
                return
            
            # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
            if not hasattr(self, 'save_dir'):
                self.save_dir = "D:/code/localtxt"
                os.makedirs(self.save_dir, exist_ok=True)
            
            # å…ˆå°è¯•è¯»å–ç°æœ‰æ–‡ä»¶å†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            filename = f"{session_id}_conversation.json"
            filepath = os.path.join(self.save_dir, filename)
            
            existing_data = {}
            if os.path.exists(filepath):
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                    logger.info(f"ğŸ“– è¯»å–ç°æœ‰ä¼šè¯æ–‡ä»¶: {filepath}")
                except Exception as e:
                    logger.warning(f"âš ï¸ è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥ï¼Œåˆ›å»ºæ–°æ–‡ä»¶: {e}")
            
            # åˆå¹¶æ•°æ®ï¼Œè€Œä¸æ˜¯è¦†ç›–
            merged_data = self.merge_session_data(existing_data, session_data)
            
            # æ·±åº¦å¤„ç†ä¼šè¯æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µå¯åºåˆ—åŒ–
            def make_json_serializable(data):
                """é€’å½’å¤„ç†æ•°æ®ä½¿å…¶å¯JSONåºåˆ—åŒ–"""
                from datetime import datetime, date
                from decimal import Decimal
                
                if isinstance(data, dict):
                    return {k: make_json_serializable(v) for k, v in data.items()}
                elif isinstance(data, list):
                    return [make_json_serializable(item) for item in data]
                elif isinstance(data, datetime):
                    return data.isoformat()
                elif isinstance(data, date):
                    return data.isoformat()
                elif isinstance(data, Decimal):
                    return float(data)
                elif hasattr(data, '__dict__'):
                    return make_json_serializable(data.__dict__)
                else:
                    return data
            
            # è½¬æ¢ä¼šè¯æ•°æ®
            serializable_data = make_json_serializable(merged_data)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(serializable_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ä¼šè¯å·²ä¿å­˜: {filepath}")
            
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨ä¿å­˜ä¼šè¯å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
    def merge_session_data(self, existing_data, new_data):
        """åˆå¹¶ä¼šè¯æ•°æ®ï¼Œä¿ç•™å†å²è®°å½•"""
        merged = existing_data.copy() if existing_data else {}
        
        # åˆå¹¶åŸºç¡€å­—æ®µ
        merged.update({
            'psql_used': new_data.get('psql_used', False),
            'query_count': new_data.get('query_count', 0),
            'last_query_time': new_data.get('last_query_time'),
            'database_understood': new_data.get('database_understood', False),
            'last_updated': new_data.get('last_updated') or new_data.get('created')
        })
        
        # åˆå¹¶å¯¹è¯å†å²ï¼ˆå»é‡åˆå¹¶ï¼‰
        existing_history = existing_data.get('conversation_history', [])
        new_history = new_data.get('conversation_history', [])
        
        # ä½¿ç”¨å»é‡æ–¹æ³•åˆå¹¶å†å²è®°å½•
        merged_history = self.merge_conversation_history(existing_history, new_history)
        
        # é™åˆ¶å†å²è®°å½•é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘100æ¡æ¶ˆæ¯ï¼‰
        max_history = 100
        if len(merged_history) > max_history:
            merged_history = merged_history[-max_history:]
        
        merged['conversation_history'] = merged_history
        
        # ç¡®ä¿æœ‰åˆ›å»ºæ—¶é—´
        if 'created' not in merged:
            merged['created'] = new_data.get('created')
        
        return merged
    def merge_conversation_history(self, existing_history, new_history):
        """åˆå¹¶å¯¹è¯å†å²ï¼Œé¿å…é‡å¤"""
        if not existing_history:
            return new_history
        
        if not new_history:
            return existing_history
        
        # ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºå”¯ä¸€æ ‡è¯†
        existing_timestamps = {msg.get('timestamp') for msg in existing_history if msg.get('timestamp')}
        
        # æ·»åŠ æ–°æ¶ˆæ¯ï¼ˆåªæ·»åŠ æ—¶é—´æˆ³ä¸å­˜åœ¨çš„æ¶ˆæ¯ï¼‰
        merged_history = existing_history.copy()
        
        for new_msg in new_history:
            if new_msg.get('timestamp') not in existing_timestamps:
                merged_history.append(new_msg)
        
        # æŒ‰æ—¶é—´æ’åº
        merged_history.sort(key=lambda x: x.get('timestamp', ''))
        
        return merged_history
    

    def contains_data_reference(self, message):
        """æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦åŒ…å«å¯¹å†å²æ•°æ®çš„å¼•ç”¨"""
        reference_keywords = [
            'ä¸Šé¢', 'åˆšæ‰', 'ä¹‹å‰', 'å†å²æ•°æ®', 'è¿™ä¸ªæ•°æ®', 'è¿™äº›æ•°æ®',
            'ä¸Šé¢è¿™ä¸ª', 'åˆšæ‰çš„', 'å‰è¿°', 'ä¸Šæ–‡', 'åˆšæ‰é‚£ä¸ª', 'åˆ†ææ•°æ®',
            'åˆ†æåˆšæ‰', 'åˆ†æä¹‹å‰', 'åˆ†æä¸Šæ–‡', 'åˆ†æè¿™äº›'
        ]
        
        message_lower = message.lower()
        for keyword in reference_keywords:
            if keyword in message_lower:
                return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«"åˆ†æ"+"æ•°æ®"çš„ç»„åˆ
        if 'åˆ†æ' in message_lower and any(word in message_lower for word in ['æ•°æ®', 'è¿™ä¸ª', 'åˆšæ‰', 'ä¸Šé¢']):
            return True
        
        return False
    def enrich_message_with_data_context(self, message, recent_data):
        """ä¸ºæ¶ˆæ¯æ·»åŠ æ•°æ®ä¸Šä¸‹æ–‡"""
        try:
            data_context = f"""
    ç”¨æˆ·æ­£åœ¨å¼•ç”¨ä¹‹å‰çš„æ•°æ®åˆ†æç»“æœã€‚æœ€è¿‘çš„æŸ¥è¯¢ä¿¡æ¯ï¼š
    - æŸ¥è¯¢æ—¶é—´ï¼š{recent_data.get('query_time', 'æœªçŸ¥')}
    - æ•°æ®é‡ï¼š{recent_data.get('data_count', 0)} æ¡è®°å½•
    - åŸå§‹é—®é¢˜ï¼š{recent_data.get('user_message', 'æœªçŸ¥')}
    å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼š{message}
    è¯·åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡è¿›è¡Œå›ç­”ï¼Œç‰¹åˆ«æ˜¯è¦å‚è€ƒä¹‹å‰çš„æ•°æ®åˆ†æç»“æœã€‚
            """.strip()
            
            return data_context
        except Exception as e:
            logger.warning(f"âš ï¸ æ·»åŠ ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return message




    def _simple_remove_duplicates(self, messages):
        """ç®€å•çš„å»é‡æ–¹æ³•ï¼ˆå¤‡ç”¨ï¼‰"""
        if not messages:
            return []
        
        seen = set()
        unique_messages = []
        
        for msg in messages:
            content = msg.get('content', '')
            role = msg.get('role', '')
            key = f"{role}:{content}"
            
            if key not in seen:
                seen.add(key)
                unique_messages.append(msg)
        
        return unique_messages

    

    







    def sanitize_json_content(self, content):
        """æ¸…ç†JSONå†…å®¹ä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
        if not content:
            return content
        
        # ç§»é™¤æˆ–è½¬ä¹‰å¯èƒ½ç ´åJSONçš„å­—ç¬¦
        content = content.replace('\\', '\\\\')  # è½¬ä¹‰åæ–œæ 
        content = content.replace('"', '\\"')    # è½¬ä¹‰åŒå¼•å·
        content = content.replace('\n', '\\n')   # è½¬ä¹‰æ¢è¡Œç¬¦
        content = content.replace('\t', '\\t')   # è½¬ä¹‰åˆ¶è¡¨ç¬¦
        content = content.replace('\r', '\\r')   # è½¬ä¹‰å›è½¦ç¬¦
        
        return content
    

    def remove_duplicate_messages(self, messages):
        """ç§»é™¤é‡å¤çš„æ¶ˆæ¯ï¼Œä¿ç•™æœ€æ–°çš„ä¸€ä¸ª"""
        if not messages:
            return []
        
        seen_content = set()
        unique_messages = []
        
        # ä»æœ€æ–°åˆ°æœ€æ—§éå†ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„å†…å®¹
        for message in reversed(messages):
            content = message.get('content', '')
            role = message.get('role', '')
            
            # åªå¯¹ç”¨æˆ·æ¶ˆæ¯è¿›è¡Œå»é‡
            if role == 'user' and content:
                if content not in seen_content:
                    seen_content.add(content)
                    unique_messages.append(message)
            else:
                # åŠ©æ‰‹æ¶ˆæ¯ç›´æ¥ä¿ç•™
                unique_messages.append(message)
        
        # æ¢å¤åŸå§‹é¡ºåº
        return list(reversed(unique_messages))
